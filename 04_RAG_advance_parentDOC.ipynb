{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XVL3ApmGPT-Y",
        "K7Tpj3PzRv5Q"
      ],
      "authorship_tag": "ABX9TyPaQMiMLuSITWXkQUfIhO/4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genie0320/langchain/blob/main/04_RAG_advance_parentDOC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 개요"
      ],
      "metadata": {
        "id": "XVL3ApmGPT-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG는 여러 과정을 거치면서 진행된다. 매 과정이 치명적으로 중요하다.\n",
        "\n",
        "- 멀티쿼리 : 대충질문해도 좋은 답변을 원하는 이들을 위해 다음에 집중해야 한다.\n",
        "- 페어런트 도큐먼트 : 앞뒤 문장을 잘 담아야 하고(chunk의 중요성)\n",
        "- 셀프쿼리(질문재해석) : 시맨틱검색 말고 쿼리가 필요한 경우\n",
        "  - 시맨틱검색이란 질문문장과 임베딩데이터의 벡터값에 따라 유사데이터를 걸러내는 것인데, 이 경우 질문의 모양이 조금만 달라져도 추출 데이터 자체가 달라진다. 이것은 '질문이 우선되는 구조'이기 때문이다.\n",
        "  이 경우, 데이터를 중심으로 사용자의 질문을 참고하여 쿼리를 날려서 정리해야 할 필요가 생길 수 있다. 이것도 고려해야 한다.\n",
        "- 타임 웨이티드 : 오래된 자료는 덜 참고했으면...\n",
        "  - 최근에 올라간 자료에 무게를 둬서 답변을 참고했으면 좋겠다."
      ],
      "metadata": {
        "id": "TDhMP6vXPV_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 멀티쿼리 리트리버\n",
        "\n",
        "사용자의 질문을 여러개의 유사질문으로 재생성하고, 각 질문에 대한 데이터를 추출해서 답변을 생성하는 방법."
      ],
      "metadata": {
        "id": "OKJkxZJHQirO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 코드"
      ],
      "metadata": {
        "id": "lYRz1PfTQhBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting"
      ],
      "metadata": {
        "id": "K7Tpj3PzRv5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colab 환경설정\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "os.environ['HF_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXg9uG_AReSG",
        "outputId": "3bbace57-6643-4c28-bcd1-a0bebc4fba84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U -q langchain pypdf sentence_transformers chromadb langchain-openai\n",
        "!pip install -q langchain pypdf sentence_transformers chromadb langchain-openai"
      ],
      "metadata": {
        "id": "ILW5zYHPRJ55"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유틸들\n",
        "import math\n",
        "import time\n",
        "\n",
        "def trace(func):\n",
        "    def wrapper():\n",
        "        start = time.time()\n",
        "        func()\n",
        "        end = time.time()\n",
        "        print(f\"{end - start:.5f} sec\")\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "qLVcfbsEaq-n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ___ setting ___\n",
        "LLM_MODEL = \"gpt-3.5-turbo\"\n",
        "MAX = 265\n",
        "TEMP = 1.5\n",
        "# _______________\n",
        "\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_compbot = OpenAI(\n",
        "    temperature=TEMP,\n",
        "    max_tokens = MAX,\n",
        "    verbose = True,\n",
        ")\n",
        "openai_chatbot = ChatOpenAI(\n",
        "    model_name = LLM_MODEL,\n",
        "    temperature=TEMP,\n",
        "    max_tokens = MAX,\n",
        "    verbose = True,\n",
        ")"
      ],
      "metadata": {
        "id": "ob-WP0rSqz7i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://n.news.naver.com/article/002/0002319323?cds=news_media_pc&type=editn'\n",
        "URLs = [\n",
        "    URL,\n",
        "    'https://n.news.naver.com/article/079/0003862456?cds=news_media_pc&type=editn'\n",
        "]\n",
        "SOURCE_folder = \"/content/drive/MyDrive/data/test\"\n",
        "SOURCE = \"/content/source/1-1그로스 해킹, 마케팅과 어떻게 다른가요_ - PUBLY.pdf\"\n",
        "\n",
        "chunk_size = 200\n",
        "\n",
        "DB_URL = '/content/drive/MyDrive/vector_upgrade/'\n",
        "MODEL_EMBED= \"jhgan/ko-sbert-nli\"\n",
        "\n",
        "llm = openai_compbot\n",
        "llm_chat = openai_chatbot"
      ],
      "metadata": {
        "id": "Nax3OGcEoGXK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ParentDocumentRetriever"
      ],
      "metadata": {
        "id": "NchMUq1GmYQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사용자 질문에 맞는 청크를 반환받고 나서, 해당 청크의 부모가 된 원본을 다시 한번 참고한다는 흐름.\n",
        "\n",
        "사용하는 이유는...\n",
        "임베딩의 특성때문이다.\n",
        "\n",
        "청크 하나당 임베딩값 한개가 만들어진다.\n",
        "따라서 청크가 너무 크면, '너무 많은 것을 담아서 다른 임베딩데이터와 구분점을 찾기가 힘든' 흐릿한 임베딩이 될 가능성이 있다.\n",
        "\n",
        "반면, 청크가 너무 작으면, '선명하긴 한데, 앞뒤 맥락이 없어서 아무짝에도 쓸모가 없는', 즉 LLM에게 제대로 된 커닝페이퍼가 되어줄 수 없는 context를 전달하게 된다.\n",
        "\n",
        "따라서... 자식청크는 작게 만들어서 일종의 '목차'처럼 명확하게 이슈를 구분해주고, 해당 이슈에 대한 사용자의 질문이 오면, 그 이슈와 관련된 '엄마' 청크를 찾아서 context가 풍부한 자료를 전달할 수 있도록 하기 위해서다.\n",
        "\n",
        "또한 엄마청크가 너무 크면 llm의 window error 가능성이 있으므로, 엄마청크도 적절한 크기로 잘라서 저장해둘 수 있다.\n",
        "\n",
        "- [x] 그럼 속도가 느려지지 않는가? > 조온나 느리다. 진짜... GPU써도 너무너무 느리다.\n",
        "- [x] 아예 처음부터 부모문서를 다 줘버리지 그래. > 임베딩의 특성상 엄마문서를 다 주면 윈도우크기를 벗어날 수도 있고, 임베딩 데이터간의 특성을 잃게 된다. 자식들은 다 개성이 뚜렷하지만, 엄마들은 다 ㅈㄹ같다는 부분에서 동일하다는 걸 생각해보자.\n",
        "\n",
        "**InMemoryStore**\n",
        "청크가 누구 자식인지를 기억해 놓는 기능.\n",
        "- [x] 어차피 메타데이터가 붙지 않아? > loader에서 읽혀진 docs는 기본 메타데이터를 갖는다. 그리고 자식이건 엄마건 그 docs를 잘라서 마련된다. (단지, 작게 잘린놈, 덜 잘린놈으로 구분되는 것) 그런데... 어느집안 자식인지는 기본 메타로 알 수 있지만, 그게 어느엄마 자식인지는 따로 연결고리를 만들어줘야 한다.\n",
        "알아보기 싫어서 예상만 하건대... 아마도, 먼저 엄마청크를 자르고, 그 청크를 작게 만들어서 자식청크를 만든다음, 엄마 이름을 추가 메타로 붙여주는 구조가 아닐까 싶다."
      ],
      "metadata": {
        "id": "s0z4hLRTmg-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "Il8FT4q9m4uS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 로딩가능성 판단.\n",
        "# file_path = '/content/drive/MyDrive/data/test/2023_디지털_분야_트렌드-게시_3.pdf'\n",
        "# file_loader = PyPDFLoader(file_path)\n",
        "# pages = file_loader.load()\n",
        "# chracter_count = len(pages[0].page_content)\n",
        "\n",
        "# if chracter_count == 50:\n",
        "#     print(f'fail to load : {file_path}')"
      ],
      "metadata": {
        "id": "NINfDcJAV6Hd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    '''\n",
        "    # 왜 쓰는지 모르겠는 방법.\n",
        "    > 텍스트로더의 경우, 문서를 하나의 객체로 반환한다.\n",
        "    하지만 pdf로더의 경우, 한 페이지를 하나의 객체로 반환한다.\n",
        "    따라서 청크 사이즈가 충분히 크다면(한 페이지를 다 들여올 수 있을만큼), 사실 Parent~는 쓸모가 없...을 것 같다.\n",
        "\n",
        "    다만, 어차피 parent~ 에서도, 뒤에 부모문서의 크기가 너무 큰 경우,\n",
        "    이를 split 해주는 기능을 넣고 있으므로...\n",
        "\n",
        "    만약 들여오는 문서가 text라면 이 기능을 활용해볼만 하나,\n",
        "    - pdf의 경우 한 페이지당 하나의 객체를 반환한다는 점.\n",
        "    - directory로더를 사용하면 어차피 한 폴더 내의 문서들을 통째로 하나의 리스트로 담아준다는 점..\n",
        "    에서 이 방식은 의미없는 방식이라고 할 수 있겠다.\n",
        "\n",
        "    즉, 이 방식은, text 문서를 여러개 (왜 때문인지 따로) 가져와서\n",
        "    다운스트림에서 사용할 수 있는 하나의 document 목록에 뭉쳐주기 위해서 이용한 고육지책이라고 할 수 있다.\n",
        "    좀 현실적인 예를 보여줄 것이지... 왜 일케 예시를 위한 예시를 보여준 것인지 당췌 이해가...\n",
        "\n",
        "    loaders = [\n",
        "        TextLoader(\"../../paul_graham_essay.txt\"),\n",
        "        TextLoader(\"../../state_of_the_union.txt\"),\n",
        "    ]\n",
        "    docs = []\n",
        "    for loader in loaders:\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    len(docs) # 54\n",
        "    '''\n",
        "    )\n",
        "\n",
        "# 재료준비\n",
        "loader = DirectoryLoader(SOURCE_folder, glob='**/*.pdf', show_progress=True, loader_cls=PyPDFLoader, use_multithreading=True)\n",
        "documents = loader.load_and_split()\n",
        "\n",
        "print(len(documents), '\\n\\n', documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO_egXhOQG7D",
        "outputId": "54544922-e013-4209-efe7-b24949245ae9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:05<00:00,  2.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35 \n",
            "\n",
            " page_content='전망\\n30  | 산은조사월보     ·런던금속거래소 구리 가격(달러/톤):8,365(‘22.12월)→9,004(’23.3 월)→8,322(6월)→8,029(10월)\\n     ·런던금속거래소 니켈 가격(달러/톤):29,886(‘22.12 월)→23,651(’23.3 월)→20,346(6월)\\n→17,903(10 월)\\n❍ 2024년 철강 및 금속가격은 글로벌 수요 둔화와 공급 확대에 따른 수급 불균형 \\n으로 하락 전망\\n-중국 정부의 부동산 부양책 35)에도 불구하고 부동산경기 침체와 인프라 투자 \\n둔화가 지속될 것으로 예상되는 가운데,미국 경기둔화로 철강 및 구리 등 금속\\n가격은 하락세 유지\\n-다만,주요 광물 생산국의 공급 확대*에 따른 가격 하방 압력에도 선진국을 \\n중심으로 한 탈탄소화 흐름이 지속됨에 따라 배터리 소재 등 관련 금속(니켈·\\n코발트 등 )수요 증가가 가격 하락을 제한\\n    * 주요 생산지인 인도네시아와 필리핀 등 동남아 , 폐루 등 남미, 콩고민주공화국 등에서 \\n알루미늄 , 니켈, 구리, 아연 등 금속 생산이 지속적으로 늘어날 전망\\n     ·미국의 ‘인플레이션 감축법(IRA:Inflation Reduction Act)36)’시행,유럽의 \\n‘RecHFT4’ocHFTt8ecHFTT6cHFT5/cHFT8’ 계획37)’등이 관련 금속인 주석,구리,니켈,아연 등의 수요 \\n증가를 뒷받침\\n35)중국 정부는 2023년‘생애 첫 주택구매자 요건’및‘주택구매제한’ 완화,‘주택담보대출 금리’인하 등 부동산 \\n경기부양책을 발표\\n36)동 법은 2022년 8월 16일 발효된 법으로 북미에서 조립되고 배터리 소재 및 부품을 미국 및 \\nFTA를 맺은 국가에서 일정 비율 이상 조달한 전기차에만 보조금을 지급하는 내용을 포함하고 \\n있으며,2030년까지 미국의 신차 판매량 중에서 전기차 비율을 50%목표\\n37)러시아 화석연료에 대한 의존도를 빠르게 줄이고 녹색 전환을 가속화 하기 위한 계획.특히,\\n2035년부터 내연기관 신차 판매가 금지되어, 전기차 배터리 관련 원자재 수요는 지속적으로 \\n증가할 전망2024년 국내경제 전망\\n2023. 12 제817호  31  2024년 국내경제 전망\\nKDB미래전략연구소 미래전략개발부\\n오세진 연구위원(ohsejin@kdb.co.kr)\\n양서영 선임연구원(scHFT22eon g@kdb.co.kr)\\n이도건 전임연구원( gdl1101@kdb.co.kr)\\nⅠ. 국내경제\\nⅡ. 민간소비\\nⅢ. 투자Ⅳ. 수출입\\nⅤ. 물가\\nⅥ. 고용\\n2024년 경제성장률은 전년도 낮은 성장률에 따른 기저효과, 수출 및 설비투자 \\n회복 등으로 2.2cHFT27수준의 완만한 성장세가 예상되나, 고금리 장기화에 따른 실물\\n경제 불안요인도 같이 확대될 것으로 전망된다.\\n그동안 하락세를 보여왔던 반도체 조정 사이클이 마무리 되며 수출도 다시 \\n회복세로 접어들고, 인공지능, 2차전지,미래차 등 첨단산업 분야에 대한 투자 \\n등이 내년도 경제성장을 견인할 것으로 예상된다.\\n그러나 고금리 장기화에 따른 가계부채 원리금 상환부담 증가,높아진 물가수준 \\n지속,가계부채 증가에 대한 경계감 등이 소비여력을 제약하여 민간소비 회복세는 \\n전년과 비슷한 2.2cHFT27에 머물 것으로 보인다.\\n건설투자는 과도한 가격 상승에 따른 조정국면 진입,고금리 장기화에 따른 \\n가계의 부동산 수요감소 ,부동산 PF수익성 악화 등이 중첩되며 건설수주가 지속적\\n으로 줄어들고 있는 상황으로 내년부터는 감소세로 접어들 것으로 보인다.\\n수출은 반도체 경기가 다시 회복세로 접어들며 증가할 것으로 예상되나, 무역장벽 \\n심화,주변국과의 경쟁 심화 등으로 첨단산업을 제외한 여타 산업의 수출 개선에는 \\n어려움이 있을 것으로 보인다.다만 경상수지는 상품수지가 증가하고, 외국인 관광객\\n증가로 서비스수지가 개선되며 전년보다 증가할 것으로 예상된다.\\n소비자물가는 고금리 장기화에 따른 수요측 물가상승 압력 약화로 증가세는 둔화\\n될 것으로 보이나,지정학적 불안,기후변화 심화,무역장벽 확대 등 물가자극 요인\\n들이 산재해 있어 인플레이션 심리를 자극할 가능성이 높을 것으로 예상된다 .\\n고용은 제조업 분야에서는 증가할 것으로 예상되나, 그동안 높은 증가세를 \\n보였던 서비스업 고용이 둔화세로 접어들며 취업자수 증가폭은 감소할 것으로 예상\\n된다.\\n*본고의 내용은 집필자 견해로 당행의 공식입장이 아님' metadata={'source': '/content/drive/MyDrive/data/test/2024년 국내경제 전망.pdf', 'page': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ko_embed = HuggingFaceEmbeddings(\n",
        "    model_name= MODEL_EMBED,\n",
        "    model_kwargs={'device' : 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings':True}\n",
        "    )\n",
        "\n",
        "# 자식구성\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300)\n",
        "\n",
        "vectordb = Chroma(\n",
        "    collection_name = 'full_documents', # 여기 이름이 왜 child_documents가 아니라 full 인지 도무지 이해가 안됨.\n",
        "    embedding_function=ko_embed\n",
        ")\n",
        "\n",
        "# 엄마구성\n",
        "# The storage layer\n",
        "store = InMemoryStore()\n",
        "\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000) # 사실 엄마 스플리터는 필요가 없을 수도 있다. 어차피 PDF에서는 페이지당 하나의 doc객체를 반환하기 때문이다.\n",
        "\n",
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore = vectordb, # 저장소는 여기고\n",
        "    docstore = store, # 임시 저장소는 여기인데,\n",
        "    parent_splitter = parent_splitter, # 엄마는 이렇게 자르고\n",
        "    child_splitter = child_splitter, # 애는 이렇게 처리해.\n",
        ")"
      ],
      "metadata": {
        "id": "XtIwBJDiZDYk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 위의 리트리버를 이용해서 로딩한 문서를 넣어준다.\n",
        "# 다른 경우와는 달리, 위에서 설정했던 스플리터를 사용해서 얘가 다 알아서 잘라주고, 부모-자식간의 연결고리도 마련한다.\n",
        "# 그래서 시간이 현기증날만큼 조온나 오래걸린다. GPU를 써도 오래걸린다. 와 짜증 진짜...\n",
        "\n",
        "@trace\n",
        "def make_context():\n",
        "    retriever.add_documents(documents, ids=None)\n",
        "\n",
        "make_context()\n",
        "len(list(store.yield_keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYSeDmy-PTq0",
        "outputId": "379582ee-68d8-465f-9f63-10aadb21defa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "165.58265 sec\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그냥 벡터DB에서 평소처럼 관련문서를 호출했을 때. 하나의 문서길이는 짧다.(자식만 불러왔을 테니까.)\n",
        "\n",
        "query = \"2024년 한국의 수출전망은 어떠한가?\"\n",
        "\n",
        "sub_docs = vectordb.similarity_search(query)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGKPP8lCUK6q",
        "outputId": "e2466f83-e0e8-473b-f1d3-49e55bd4fac9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lens = []\n",
        "for d in sub_docs:\n",
        "    lens.append(len(d.page_content))\n",
        "\n",
        "print(lens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX4b6hf9o64a",
        "outputId": "0e3ebfaa-60e3-492d-efd6-9d9629c930fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[67, 275, 283, 279]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그러나 이번에 만든 parent~ 리트리버를 사용해서 문서를 가져왔을 때는 문서 하나의 길이는 훨씬 길다는 걸 알 수 있다.\n",
        "# 자식을 끄나풀 삼아 엄마를 잡아왔기때문.\n",
        "\n",
        "parent_docs = retriever.get_relevant_documents(query)\n",
        "len(parent_docs[0].page_content)\n",
        "\n",
        "lens = []\n",
        "for d in sub_docs:\n",
        "    lens.append(len(d.page_content))\n",
        "\n",
        "print(lens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm8KBwn5m7zJ",
        "outputId": "2acb4feb-4a73-4599-8c98-dabbecfbd19c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lens = []\n",
        "for d in parent_docs:\n",
        "    lens.append(len(d.page_content))\n",
        "\n",
        "print(lens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVwZn3MVo9Ro",
        "outputId": "8b194241-f4f8-4567-c6fd-5a1f5b8aabc4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[67, 552, 924, 966]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [ ] 근데 부모는 덩치가 큰데... 많은 부모가 잡혀왔을 때는 이걸 어떻게 context로 전달해줄 수 있나?\n",
        "- [ ] 나는 왜 항상 내가 정했던 chunk 사이즈보다 크게 잘라지는거지...?"
      ],
      "metadata": {
        "id": "aUMLMAVzn4fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "\n",
        "# logging.basicConfig()\n",
        "# logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "bbt5QAWFkxyL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
        "# len(unique_docs)"
      ],
      "metadata": {
        "id": "666WdF0HlIMd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_docs"
      ],
      "metadata": {
        "id": "EDwJr_YSle99"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}