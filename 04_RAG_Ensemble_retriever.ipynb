{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk9meGfCCj2DdoPOUX9SIl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genie0320/langchain/blob/main/04_RAG_Ensemble_retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever"
      ],
      "metadata": {
        "id": "SqPa9SjtqcwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble = (sparse + Dense) + reorder**\n",
        "\n",
        "sparse는 키워드를 중심으로 정리하고 검색한다. dense는 문장을 중심으로 정리하고 검색한다. 즉 하나의 키워드라면 sparse가 더 찾을 수 있고, 그걸 어떤 키워드를 문맥적으로 파악하는 것은 dense이다.\n",
        "그래서 sparse는 이음동의어를 찾을 수 없지만, dense는 이음동의어를 처리할 수 있게 된다.\n",
        "Ensemble Retriever : 위의 2가지 방식을 써서... 더 정확한 답을 얻을 수 있도록 한다.  "
      ],
      "metadata": {
        "id": "aAsOW9T8j2yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Long context Reorder : 보통 일을 할 때, 도입부에 신경쓰고 마무리에 신경 좀 쓰면 최소노력으로 최대효과를 볼 수 있는데 AI가 똑똑하게도, 주어진 문서 중에 첫 3개정도와 마지막 몇개를 가장 집중적으로 참고한다고 한다(U자 모양) 그래서.. 중요한 문서일 수록 이 앞뒤에 배치해줄 수 있도록 하는 기술."
      ],
      "metadata": {
        "id": "-6sACMfdm1ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colab 환경설정\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "os.environ['HF_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "from google.colab import drive as gd\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nj2MsACEort7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --quiet icecream > /dev/null"
      ],
      "metadata": {
        "id": "wy22SDYmqK7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "from icecream import ic\n",
        "\n",
        "def pp(object):\n",
        "  ppr = pprint.PrettyPrinter(\n",
        "      # indent=40,\n",
        "      width=80\n",
        "      )\n",
        "  return ppr.pprint(object)"
      ],
      "metadata": {
        "id": "Z_gxlLIkqIbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Clear all output cells\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "TtAk0qCEWROX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensenble"
      ],
      "metadata": {
        "id": "q4Y-36pJqWgR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-BXEShkjXMe"
      },
      "outputs": [],
      "source": [
        "pip install -q langchain langchain-openai pypdf sentence-transformers chromadb  faiss-cpu -U rank_bm25 > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "kp6kJfbzmz1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ___ setting ___\n",
        "LLM_MODEL = \"gpt-3.5-turbo\"\n",
        "MAX = 265\n",
        "TEMP = 1.5\n",
        "# _______________\n",
        "\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_compbot = OpenAI(\n",
        "    temperature=TEMP,\n",
        "    max_tokens = MAX,\n",
        "    verbose = True,\n",
        ")\n",
        "openai_chatbot = ChatOpenAI(\n",
        "    model_name = LLM_MODEL,\n",
        "    temperature=TEMP,\n",
        "    max_tokens = MAX,\n",
        "    verbose = True,\n",
        ")\n",
        "\n",
        "ko_embed = HuggingFaceEmbeddings(\n",
        "    model_name=\"jhgan/ko-sbert-nli\",\n",
        "    model_kwargs={'device' : 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings':True}\n",
        "    )"
      ],
      "metadata": {
        "id": "0SJ0KR8QoTY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "\n",
        "SOURCE_folder = \"/content/drive/MyDrive/data/brand\"\n",
        "SOURCE = \"/content/source/1-1그로스 해킹, 마케팅과 어떻게 다른가요_ - PUBLY.pdf\"\n",
        "\n",
        "loader = DirectoryLoader(\n",
        "    SOURCE_folder,\n",
        "    glob='**/*.pdf',\n",
        "    show_progress=True,\n",
        "    loader_cls=PyPDFLoader,\n",
        "    )\n",
        "\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "# 이건 여러번의 로더 호출을 통해, 각 문서가 각각의 로더객체에 담겼을 경우,\n",
        "# 이 모두를 합해서 하나의 doc 객체로 만들어서 downstream에서 이용하기 위해 합쳐주는 기능.\n",
        "# 따라서... 우리는 디렉토리 로더로 어차피 하나의 객체에 모든 문서를 '페이지별'로 담았다.\n",
        "# 이 pypdf는 내부적으로 리컬시브~를 쓴다고 한다.\n",
        "# docs = []\n",
        "# for page in pages:\n",
        "#     docs.append(page.page_content)\n",
        "\n",
        "ic(len(pages))"
      ],
      "metadata": {
        "id": "jkKTlw4uocKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이걸 문서별로 덩어리로 만들고 싶다. 즉 불러온 문서가 3개라면, 객체속의 요소도 3개만 존재하도록.\n",
        "# keys = []\n",
        "# for page in pages:\n",
        "#     source = page.metadata['source']\n",
        "#     if source not in keys:\n",
        "#         keys.append(page.metadata['source'])\n",
        "# keys\n",
        "\n",
        "# 그러나 지금은 일단 포기\n",
        "\n",
        "lens = []\n",
        "for page in pages:\n",
        "    lens.append(len(page.page_content))\n",
        "\n",
        "print(sorted(lens, reverse = True), end=' ')\n",
        "print('\\n','*'*100, '\\n')\n",
        "print(min(lens), max(lens))\n",
        "\n",
        "docs = pages"
      ],
      "metadata": {
        "id": "K26pGycn3rAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 적정한 길이로 잘라주고\n",
        "rc_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=350,\n",
        "    chunk_overlap=30,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "# chunks = rc_splitter.create_documents(docs) # 옵션을 줄 수 있다. []을 받는다.\n",
        "chunks = rc_splitter.split_documents(docs) # 옵션을 줄 수 있다. []을 받는다.\n",
        "\n",
        "len(chunks)\n",
        "\n",
        "lens = []\n",
        "for chunk in chunks:\n",
        "    lens.append(len(chunk.page_content))\n",
        "\n",
        "print(sorted(lens, reverse = True), end=' ')\n",
        "print('\\n','*'*100, '\\n')\n",
        "print(min(lens), max(lens))"
      ],
      "metadata": {
        "id": "iNleX-L0pH9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for Sparse retriever\n",
        "bm25_ret = BM25Retriever.from_documents(chunks)\n",
        "bm25_ret.k=2 # 여기는 선언방식이 진짜 특이하다.\n",
        "\n",
        "#for Dense retriever\n",
        "embedding = ko_embed\n",
        "faiss_db = FAISS.from_documents(chunks, embedding)\n",
        "faiss_ret = faiss_db.as_retriever(search_kwargs={'k':2})\n",
        "\n",
        "#init ensemble retriever\n",
        "ensemble_ret = EnsembleRetriever(\n",
        "    retrievers = [bm25_ret, faiss_ret], weight=[0.5,0.5]\n",
        ")"
      ],
      "metadata": {
        "id": "BvZXN1KqJf65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 앙상블은 양쪽 리트리버에서 결과를 가져와서\n",
        "# 순위별로 재정렬도 해준다.\n",
        "\n",
        "query = '브랜드 네이밍과 인스타그램'\n",
        "\n",
        "doc = ensemble_ret.invoke(query)\n",
        "\n",
        "for i in docs:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "E54Go-CFP0n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_faiss = faiss_ret.invoke(query)\n",
        "\n",
        "for i in docs_faiss:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "xhpWPGZkShoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_bm25 = bm25_ret.invoke(query)\n",
        "\n",
        "for i in docs_bm25:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "xjOishHiTO1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm = openai_compbot,\n",
        "    chain_type = 'stuff',\n",
        "    retriever = faiss_ret,\n",
        "    return_source_documents = True\n",
        ")\n",
        "\n",
        "result_faiss = qa(query)\n",
        "print(result_faiss['result'])\n",
        "\n",
        "for i in result_faiss['source_documents']:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "gwbSGUfIUKrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm = openai_compbot,\n",
        "    chain_type = 'stuff',\n",
        "    retriever = bm25_ret,\n",
        "    return_source_documents = True\n",
        ")\n",
        "\n",
        "result_bm25 = qa(query)\n",
        "print(result_bm25['result'])\n",
        "\n",
        "for i in result_bm25['source_documents']:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "oI91zafyU7Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm = openai_compbot,\n",
        "    chain_type = 'stuff',\n",
        "    retriever = ensemble_ret,\n",
        "    return_source_documents = True\n",
        ")\n",
        "\n",
        "result = qa(query)\n",
        "print(result['result'])\n",
        "\n",
        "for i in result['source_documents']:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "f9dYVcVWTdj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3개의 리트리버를 비교해보면, sparse에서 가장 키워드를 잘 찾아오고(당연한 이야기지만) 문맥적으로는 dense에서 찾는 것을 알 수 있고, 그 순서대로 ensemble에 반영되어 문장이 만들어진다는 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "HaaWry1nVKL2"
      }
    }
  ]
}