{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XVL3ApmGPT-Y"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnDJ3dUg4s4TG4YRUIrXmX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genie0320/langchain/blob/main/04_RAG_Time_weighted_vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 개요"
      ],
      "metadata": {
        "id": "XVL3ApmGPT-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG는 여러 과정을 거치면서 진행된다. 매 과정이 치명적으로 중요하다.\n",
        "\n",
        "- 멀티쿼리 : 대충질문해도 좋은 답변을 원하는 이들을 위해 다음에 집중해야 한다.\n",
        "- 페어런트 도큐먼트 : 앞뒤 문장을 잘 담아야 하고(chunk의 중요성)\n",
        "- 셀프쿼리(질문재해석) : 시맨틱검색 말고 쿼리가 필요한 경우\n",
        "  - 시맨틱검색이란 질문문장과 임베딩데이터의 벡터값에 따라 유사데이터를 걸러내는 것인데, 이 경우 질문의 모양이 조금만 달라져도 추출 데이터 자체가 달라진다. 이것은 '질문이 우선되는 구조'이기 때문이다.\n",
        "  이 경우, 데이터를 중심으로 사용자의 질문을 참고하여 쿼리를 날려서 정리해야 할 필요가 생길 수 있다. 이것도 고려해야 한다.\n",
        "- 타임 웨이티드 : 오래된 자료는 덜 참고했으면...\n",
        "  - 최근에 올라간 자료에 무게를 둬서 답변을 참고했으면 좋겠다."
      ],
      "metadata": {
        "id": "TDhMP6vXPV_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time-weighted vector Retriever\n",
        "\n",
        "제목 그 자체가 스포인데, 추출시 데이터의 신선도를 반영하는 리트리버이다.\n",
        "'시간이 지난만큼' 패널티를 주자...는 것.\n",
        "\n",
        "[ 시맨틱유사성 + (1.0 - 경과시간에 따른 부패도) ]\n",
        "따라서 만약 부패도반영을 1.0을 해버리면... 걍 일반 리트리버랑 똑같아지는거지.\n",
        "\n",
        "- [ ] 그럼... 그냥 self~ 는 시간을 전혀 고려하지 않는다는건가? '최신에'라고 쿼리에 넣어도???\n",
        "- [ ] 그럼 메타를 붙일 때 시간값을 추가해주면 될텐데... 등록시간 기준인거야 아니면 데이터 그 자체의 신선도인거야? 예를 들어... 2024년에 만들어진 1999년의 경제보고서의 데이터가 우선되는 것인지, 2023년에 만들어진 2022년의 보고서데이터가 우선되는 것인지? 아마 옵션으로 주는게 있을텐데...\n",
        "- [ ] 데이터를 넣어 줄 때 이 모든 걸 다 찾아서 넣어줘야 한다고? 장난해...?\n",
        "\n",
        "\n",
        "### 결론.\n",
        "**decay rate와 relevance 사이의 절묘한 줄타기가 필요한 부분이당**"
      ],
      "metadata": {
        "id": "OKJkxZJHQirO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting"
      ],
      "metadata": {
        "id": "K7Tpj3PzRv5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colab 환경설정\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "os.environ['HF_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXg9uG_AReSG",
        "outputId": "9410f6c4-18fe-4c11-a0ff-1fe7e69ac949"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U -q langchain pypdf sentence_transformers chromadb langchain-openai\n",
        "!pip install -q langchain langchain-openai sentence_transformers"
      ],
      "metadata": {
        "id": "ILW5zYHPRJ55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9ca1e1-0b87-4b05-cffc-f07ec5f2a79c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 유틸들\n",
        "import math\n",
        "import time\n",
        "\n",
        "def trace(func):\n",
        "    def wrapper():\n",
        "        start = time.time()\n",
        "        func()\n",
        "        end = time.time()\n",
        "        print(f\"{end - start:.5f} sec\")\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "qLVcfbsEaq-n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ___ setting ___\n",
        "LLM_MODEL = \"gpt-3.5-turbo\"\n",
        "MAX = 265\n",
        "TEMP = 1.5\n",
        "# _______________\n",
        "\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "openai_compbot = OpenAI(\n",
        "    temperature=TEMP,\n",
        "    max_tokens = MAX,\n",
        "    verbose = True,\n",
        ")\n",
        "openai_chatbot = ChatOpenAI(\n",
        "    model_name = LLM_MODEL,\n",
        "    temperature=TEMP,\n",
        "    max_tokens = MAX,\n",
        "    verbose = True,\n",
        ")\n",
        "\n",
        "MODEL_EMBED= \"jhgan/ko-sbert-nli\"\n",
        "\n",
        "ko_embed = HuggingFaceEmbeddings(\n",
        "    model_name= MODEL_EMBED,\n",
        "    model_kwargs={'device' : 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings':True}\n",
        "    )\n",
        "\n",
        "llm = openai_compbot\n",
        "llm_chat = openai_chatbot"
      ],
      "metadata": {
        "id": "ob-WP0rSqz7i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://n.news.naver.com/article/002/0002319323?cds=news_media_pc&type=editn'\n",
        "URLs = [\n",
        "    URL,\n",
        "    'https://n.news.naver.com/article/079/0003862456?cds=news_media_pc&type=editn'\n",
        "]\n",
        "SOURCE_folder = \"/content/drive/MyDrive/data/test\"\n",
        "SOURCE = \"/content/source/1-1그로스 해킹, 마케팅과 어떻게 다른가요_ - PUBLY.pdf\"\n",
        "\n",
        "DB_URL = '/content/drive/MyDrive/vector_upgrade/'"
      ],
      "metadata": {
        "id": "Nax3OGcEoGXK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 코드"
      ],
      "metadata": {
        "id": "lYRz1PfTQhBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q faiss-gpu # GPU사용시\n",
        "# !pip install -U -q faiss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdRtvtRnPg-v",
        "outputId": "93d2418e-5779-4a55-ca58-09e9dc285588"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time-weighted vector Retriever"
      ],
      "metadata": {
        "id": "NchMUq1GmYQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "import faiss\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore # 아... 이놈 나왔을 때 좋은 기억이 없는데 오늘도 평탄하진 않겠네...ㅡ.ㅡ;\n",
        "from langchain.schema import Document # 이건 데이터에 대한 잔소리를 추가해줄 때 사용하는 모양.\n",
        "from langchain.retrievers import TimeWeightedVectorStoreRetriever"
      ],
      "metadata": {
        "id": "WT_J0r-5WDTu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 사이즈는 faiss에서 요구하는 값이며 모델마다 다를 수 있다.\n",
        "# 현재 사용중인 'MODEL_EMBED= \"jhgan/ko-sbert-nli\"' 모델의 경우, https://huggingface.co/jhgan/ko-sbert-nli 에서\n",
        "# Full Model Architecture > SentenceTransformer > Pooling > 'word_embedding_dimension': 768 임을 확인할 수 있다.\n",
        "embedding_size = 768\n",
        "index = faiss.IndexFlatL2(embedding_size) # L2는 뭐지? L2캐시 이야기인가? 아는게 그거밖에 없어서.\n",
        "vectorstore = FAISS(\n",
        "    ko_embed,\n",
        "    index,\n",
        "    InMemoryDocstore({}), # 역시나 임시저장소. 하지만 나중에 persist path랑 바꿔끼우기는 좋겠군.\n",
        "    {} # 이 특이한 빈자리는 머지?\n",
        "    )"
      ],
      "metadata": {
        "id": "2YwauP6IWxk0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = TimeWeightedVectorStoreRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    decay_rate = 0.99, # 시간가중치 최대\n",
        "    # decay_rate = 0.01,\n",
        "    k=1\n",
        ")"
      ],
      "metadata": {
        "id": "_pBn6SdoaYsR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yesterday = datetime.now() - timedelta(days=1)\n",
        "retriever.add_documents([\n",
        "    Document(\n",
        "        page_content = '영어는 훌륭합니다',\n",
        "        metadata={'last_accessed_at' : yesterday}\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "retriever.add_documents([\n",
        "    Document(\n",
        "        page_content='한국어는 훌륭합니다'\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07YAbOkgcswB",
        "outputId": "14ac8191-c9e6-4630-cf3e-cccadd190731"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['29772347-4eea-40b2-8e4c-378c0a8f0672']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents('영어가 좋아요')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-nJ_54NeEGo",
        "outputId": "224684a5-7435-4f5e-b47c-a642e29fb686"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='한국어는 훌륭합니다', metadata={'last_accessed_at': datetime.datetime(2024, 2, 15, 1, 55, 10, 670228), 'created_at': datetime.datetime(2024, 2, 15, 1, 55, 8, 670931), 'buffer_idx': 1})]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "74JpeDAUaGeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To persist the data, you need to use a different type of DocumentStore, such as PyPDFDocumentStore or MongoDBDocumentStore. These document stores store the data on disk or in a database, so it is not lost when you reconnect Colab.\n",
        "\n",
        "Here is an example of how to use PyPDFDocumentStore:\n",
        "\n",
        " ```python\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitters import CharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import MyScale, MyScaleSettings\n",
        "from langchain.docstore.pydocumentstore import PyPDFDocumentStore\n",
        "\n",
        "# Load documents from a PDF file.\n",
        "loader = PyPDFLoader(\"my_documents.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "# Create a PyPDFDocumentStore.\n",
        "docstore = PyPDFDocumentStore(\n",
        "    documents=documents,\n",
        "    text_splitter=CharacterTextSplitter(),\n",
        "    embeddings=MyScale(MyScaleSettings(embedding_function=\"openai\")),\n",
        ")\n",
        "\n",
        "# Add a new document.\n",
        "new_document = Document(page_content=\"This is a new document.\")\n",
        "docstore.add_documents(new_document)\n",
        "\n",
        "# Close the document store to save the data to disk.\n",
        "docstore.close()\n",
        "```\n",
        "Use code with caution\n",
        "Once you have closed the document store, the data will be saved to disk and you can access it again later, even if you reconnect Colab."
      ],
      "metadata": {
        "id": "v5_QC3Rka3sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 위의 리트리버를 이용해서 로딩한 문서를 넣어준다.\n",
        "# 다른 경우와는 달리, 위에서 설정했던 스플리터를 사용해서 얘가 다 알아서 잘라주고, 부모-자식간의 연결고리도 마련한다.\n",
        "# 그래서 시간이 현기증날만큼 조온나 오래걸린다. GPU를 써도 오래걸린다. 와 짜증 진짜...\n",
        "\n",
        "@trace\n",
        "# def make_context():\n",
        "#     retriever.add_documents(documents, ids=None)\n",
        "\n",
        "# make_context()\n",
        "# len(list(store.yield_keys()))"
      ],
      "metadata": {
        "id": "ZYSeDmy-PTq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "\n",
        "# logging.basicConfig()\n",
        "# logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "bbt5QAWFkxyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
        "# len(unique_docs)"
      ],
      "metadata": {
        "id": "666WdF0HlIMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_docs"
      ],
      "metadata": {
        "id": "EDwJr_YSle99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.youtube.com/watch?v=wQEl0GGxPcM&t=306"
      ],
      "metadata": {
        "id": "Kp_WwJ63u2aH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}