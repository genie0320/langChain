{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaVQLJ3NprAzrMUscQv8P2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genie0320/langchain/blob/main/04_RAG_Ensemble_retriever_longContext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever"
      ],
      "metadata": {
        "id": "SqPa9SjtqcwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble = (sparse + Dense) + reorder**\n",
        "\n",
        "sparse는 키워드를 중심으로 정리하고 검색한다. dense는 문장을 중심으로 정리하고 검색한다. 즉 하나의 키워드라면 sparse가 더 찾을 수 있고, 그걸 어떤 키워드를 문맥적으로 파악하는 것은 dense이다.\n",
        "그래서 sparse는 이음동의어를 찾을 수 없지만, dense는 이음동의어를 처리할 수 있게 된다.\n",
        "Ensemble Retriever : 위의 2가지 방식을 써서... 더 정확한 답을 얻을 수 있도록 한다.  "
      ],
      "metadata": {
        "id": "aAsOW9T8j2yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Long context Reorder : 보통 일을 할 때, 도입부에 신경쓰고 마무리에 신경 좀 쓰면 최소노력으로 최대효과를 볼 수 있는데 AI가 똑똑하게도, 주어진 문서 중에 첫 3개정도와 마지막 몇개를 가장 집중적으로 참고한다고 한다(U자 모양) 그래서.. 중요한 문서일 수록 이 앞뒤에 배치해줄 수 있도록 하는 기술."
      ],
      "metadata": {
        "id": "-6sACMfdm1ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colab 환경설정\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "os.environ['HF_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nj2MsACEort7",
        "outputId": "a02263b9-0404-4cde-e0bb-03c2da70cc5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --quiet icecream > /dev/null"
      ],
      "metadata": {
        "id": "wy22SDYmqK7y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "from icecream import ic\n",
        "\n",
        "def pp(object):\n",
        "  ppr = pprint.PrettyPrinter(\n",
        "      # indent=40,\n",
        "      width=80\n",
        "      )\n",
        "  return ppr.pprint(object)"
      ],
      "metadata": {
        "id": "Z_gxlLIkqIbp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Clear all output cells\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "TtAk0qCEWROX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensenble"
      ],
      "metadata": {
        "id": "q4Y-36pJqWgR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C-BXEShkjXMe",
        "outputId": "c0e94925-9833-4295-b3f7-6424c6724c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install -q langchain langchain-openai pypdf sentence-transformers chromadb  faiss-cpu -U rank_bm25 > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "kp6kJfbzmz1d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ___ setting ___\n",
        "LLM_MODEL = \"gpt-3.5-turbo\"\n",
        "MAX = 1000\n",
        "TEMP = 1.5\n",
        "# _______________\n",
        "\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_compbot = OpenAI(\n",
        "    temperature=TEMP,\n",
        "    max_tokens = MAX,\n",
        "    verbose = True,\n",
        ")\n",
        "openai_chatbot = ChatOpenAI(\n",
        "    model_name = LLM_MODEL,\n",
        "    temperature=TEMP,\n",
        "    max_tokens = MAX,\n",
        "    verbose = True,\n",
        ")\n",
        "\n",
        "ko_embed = HuggingFaceEmbeddings(\n",
        "    model_name=\"jhgan/ko-sbert-nli\",\n",
        "    model_kwargs={'device' : 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings':True}\n",
        "    )"
      ],
      "metadata": {
        "id": "0SJ0KR8QoTY_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "\n",
        "SOURCE_folder = \"/content/drive/MyDrive/data/brand\"\n",
        "SOURCE = \"/content/source/1-1그로스 해킹, 마케팅과 어떻게 다른가요_ - PUBLY.pdf\"\n",
        "\n",
        "loader = DirectoryLoader(\n",
        "    SOURCE_folder,\n",
        "    glob='**/*.pdf',\n",
        "    show_progress=True,\n",
        "    loader_cls=PyPDFLoader,\n",
        "    )\n",
        "\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "# 이건 여러번의 로더 호출을 통해, 각 문서가 각각의 로더객체에 담겼을 경우,\n",
        "# 이 모두를 합해서 하나의 doc 객체로 만들어서 downstream에서 이용하기 위해 합쳐주는 기능.\n",
        "# 따라서... 우리는 디렉토리 로더로 어차피 하나의 객체에 모든 문서를 '페이지별'로 담았다.\n",
        "# 이 pypdf는 내부적으로 리컬시브~를 쓴다고 한다.\n",
        "# docs = []\n",
        "# for page in pages:\n",
        "#     docs.append(page.page_content)\n",
        "\n",
        "ic(len(pages))"
      ],
      "metadata": {
        "id": "jkKTlw4uocKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이걸 문서별로 덩어리로 만들고 싶다. 즉 불러온 문서가 3개라면, 객체속의 요소도 3개만 존재하도록.\n",
        "# keys = []\n",
        "# for page in pages:\n",
        "#     source = page.metadata['source']\n",
        "#     if source not in keys:\n",
        "#         keys.append(page.metadata['source'])\n",
        "# keys\n",
        "\n",
        "# 그러나 지금은 일단 포기\n",
        "\n",
        "lens = []\n",
        "for page in pages:\n",
        "    lens.append(len(page.page_content))\n",
        "\n",
        "print(sorted(lens, reverse = True), end=' ')\n",
        "print('\\n','*'*100, '\\n')\n",
        "print(min(lens), max(lens))\n",
        "\n",
        "docs = pages"
      ],
      "metadata": {
        "id": "K26pGycn3rAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 적정한 길이로 잘라주고\n",
        "rc_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=350,\n",
        "    chunk_overlap=30,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "# chunks = rc_splitter.create_documents(docs) # 옵션을 줄 수 있다. []을 받는다.\n",
        "chunks = rc_splitter.split_documents(docs) # 옵션을 줄 수 있다. []을 받는다.\n",
        "\n",
        "len(chunks)\n",
        "\n",
        "lens = []\n",
        "for chunk in chunks:\n",
        "    lens.append(len(chunk.page_content))\n",
        "\n",
        "print(sorted(lens, reverse = True), end=' ')\n",
        "print('\\n','*'*100, '\\n')\n",
        "print(min(lens), max(lens))"
      ],
      "metadata": {
        "id": "iNleX-L0pH9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for Sparse retriever\n",
        "bm25_ret = BM25Retriever.from_documents(chunks)\n",
        "bm25_ret.k=2 # 여기는 선언방식이 진짜 특이하다.\n",
        "\n",
        "#for Dense retriever\n",
        "embedding = ko_embed\n",
        "faiss_db = FAISS.from_documents(chunks, embedding)\n",
        "faiss_ret = faiss_db.as_retriever(search_kwargs={'k':2})\n",
        "\n",
        "#init ensemble retriever\n",
        "ensemble_ret = EnsembleRetriever(\n",
        "    retrievers = [bm25_ret, faiss_ret], weight=[0.5,0.5]\n",
        ")"
      ],
      "metadata": {
        "id": "BvZXN1KqJf65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 앙상블은 양쪽 리트리버에서 결과를 가져와서\n",
        "# 순위별로 재정렬도 해준다.\n",
        "\n",
        "query = '브랜드 네이밍과 인스타그램'\n",
        "\n",
        "doc = ensemble_ret.invoke(query)\n",
        "\n",
        "for i in docs:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "E54Go-CFP0n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_faiss = faiss_ret.invoke(query)\n",
        "\n",
        "for i in docs_faiss:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "xhpWPGZkShoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_bm25 = bm25_ret.invoke(query)\n",
        "\n",
        "for i in docs_bm25:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "xjOishHiTO1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm = openai_compbot,\n",
        "    chain_type = 'stuff',\n",
        "    retriever = faiss_ret,\n",
        "    return_source_documents = True\n",
        ")\n",
        "\n",
        "result_faiss = qa(query)\n",
        "print(result_faiss['result'])\n",
        "\n",
        "for i in result_faiss['source_documents']:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "gwbSGUfIUKrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm = openai_compbot,\n",
        "    chain_type = 'stuff',\n",
        "    retriever = bm25_ret,\n",
        "    return_source_documents = True\n",
        ")\n",
        "\n",
        "result_bm25 = qa(query)\n",
        "print(result_bm25['result'])\n",
        "\n",
        "for i in result_bm25['source_documents']:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "oI91zafyU7Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm = openai_compbot,\n",
        "    chain_type = 'stuff',\n",
        "    retriever = ensemble_ret,\n",
        "    return_source_documents = True\n",
        ")\n",
        "\n",
        "result = qa(query)\n",
        "print(result['result'])\n",
        "\n",
        "for i in result['source_documents']:\n",
        "    print(i.metadata)\n",
        "    print(\"**********************\")\n",
        "    print(i.page_content)\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "f9dYVcVWTdj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3개의 리트리버를 비교해보면, sparse에서 가장 키워드를 잘 찾아오고(당연한 이야기지만) 문맥적으로는 dense에서 찾는 것을 알 수 있고, 그 순서대로 ensemble에 반영되어 문장이 만들어진다는 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "HaaWry1nVKL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long Context Reorder"
      ],
      "metadata": {
        "id": "pFSOoWqMZl6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain, StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_transformers import (\n",
        "    LongContextReorder,\n",
        ")\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "texts = [\n",
        "    \"바스켓볼은 훌륭한 스포츠입니다.\",\n",
        "    \"플라이 미 투 더 문은 제가 가장 좋아하는 노래 중 하나입니다.\",\n",
        "    \"셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
        "    \"보스턴 셀틱스에 관한 문서입니다.\", \"보스턴 셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
        "    \"저는 영화 보러 가는 것을 좋아해요\",\n",
        "    \"보스턴 셀틱스가 20점차로 이겼어요\",\n",
        "    \"이것은 그냥 임의의 텍스트입니다.\",\n",
        "    \"엘든 링은 지난 15 년 동안 최고의 게임 중 하나입니다.\",\n",
        "    \"L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.\",\n",
        "    \"래리 버드는 상징적 인 NBA 선수였습니다.\",\n",
        "]\n",
        "\n",
        "# Create a retriever\n",
        "retriever = Chroma.from_texts(texts, embedding=ko_embed).as_retriever(\n",
        "    search_kwargs={\"k\": 10}\n",
        ")\n",
        "query = \"셀틱스에 대해 어떤 이야기를 들려주시겠어요?\"\n",
        "\n",
        "# Get relevant documents ordered by relevance score\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "docs"
      ],
      "metadata": {
        "id": "53bnfKrCAPxj",
        "outputId": "d1aa4139-6900-45da-8745-1b1615db2656",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='보스턴 셀틱스에 관한 문서입니다.'),\n",
              " Document(page_content='보스턴 셀틱스에 관한 문서입니다.'),\n",
              " Document(page_content='셀틱스는 제가 가장 좋아하는 팀입니다.'),\n",
              " Document(page_content='셀틱스는 제가 가장 좋아하는 팀입니다.'),\n",
              " Document(page_content='L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.'),\n",
              " Document(page_content='L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.'),\n",
              " Document(page_content='이것은 그냥 임의의 텍스트입니다.'),\n",
              " Document(page_content='이것은 그냥 임의의 텍스트입니다.'),\n",
              " Document(page_content='보스턴 셀틱스는 제가 가장 좋아하는 팀입니다.'),\n",
              " Document(page_content='보스턴 셀틱스는 제가 가장 좋아하는 팀입니다.')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reordering = LongContextReorder()\n",
        "reordered_docs = reordering.transform_documents(docs)\n",
        "\n",
        "# Confirm that the 4 relevant documents are at beginning and end.\n",
        "reordered_docs\n",
        "\n",
        "# 순서를 잘 살펴보면... 질문과 관련된 문서인 '셀틱스'관련 문서들은 앞과 뒤로 밀어 넣은 것을 알 수 있다.\n",
        "# 그리고 그런지 아닌지는 잘 모르겠지만, 여튼...'기분'과 관련된 자료를 가장 먼저 배치했음을 알 수 있다."
      ],
      "metadata": {
        "id": "05PAhjOcaG0k",
        "outputId": "5fe73d51-b119-4a20-ff3f-90dea0cd6db4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='보스턴 셀틱스에 관한 문서입니다.'),\n",
              " Document(page_content='셀틱스는 제가 가장 좋아하는 팀입니다.'),\n",
              " Document(page_content='L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.'),\n",
              " Document(page_content='이것은 그냥 임의의 텍스트입니다.'),\n",
              " Document(page_content='보스턴 셀틱스는 제가 가장 좋아하는 팀입니다.'),\n",
              " Document(page_content='보스턴 셀틱스는 제가 가장 좋아하는 팀입니다.'),\n",
              " Document(page_content='이것은 그냥 임의의 텍스트입니다.'),\n",
              " Document(page_content='L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.'),\n",
              " Document(page_content='셀틱스는 제가 가장 좋아하는 팀입니다.'),\n",
              " Document(page_content='보스턴 셀틱스에 관한 문서입니다.')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain, StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'YOUR_API_KEY'\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "document_prompt = PromptTemplate(\n",
        "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
        ")\n",
        "\n",
        "template = \"\"\"Given this text extracts:\n",
        "-----\n",
        "{context}\n",
        "-----\n",
        "Please answer the following question:\n",
        "{query}\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"context\", \"query\"]\n",
        ")\n",
        "# openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
        "\n",
        "# 여기서는 '커스텀'체인을 구성해줘야 한다고 한다.\n",
        "llm_chain = LLMChain(llm=openai_chatbot, prompt=prompt)\n",
        "chain = StuffDocumentsChain(\n",
        "    llm_chain=llm_chain,\n",
        "    document_prompt=document_prompt,\n",
        "    document_variable_name=\"context\" # 컨텍스트라는 이름으로 돌아오는 애가 바로 저 위에 context라는 걸 알려주는 문구\n",
        ")\n"
      ],
      "metadata": {
        "id": "7bdFMZGpaQP6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reordered_result = chain.run(input_documents=reordered_docs, query=query)\n",
        "result = chain.run(input_documents=docs, query=query)\n",
        "\n",
        "print(reordered_result)\n",
        "print(\"-\"*100)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "GWSDZXEBaRgq",
        "outputId": "8f93b002-0cde-4bae-fbe5-529c925db96d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "셀틱스는 제작자가 가장 좋아하는 팀입니다. L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "셀틱스는 제가 가장 좋아하는 팀입니다. L. 코넷은 최고의 셀틱스 선수 중 한 명입니다. 보스턴 셀틱스는 제가 가장 좋아하는 팀입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhAaUwYSchSI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}